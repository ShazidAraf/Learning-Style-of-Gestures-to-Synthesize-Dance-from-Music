{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import imageio\n",
    "from pathlib import Path\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from custom_utils.datastft import single_spectrogram\n",
    "\n",
    "if torch.cuda.device_count()>0:\n",
    "    torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from model_utils import PoseMusicDataset_new, CNNFeat, MDNRNN, Nelson_model\n",
    "from model_utils import save_checkpoint, load_checkpoint,load_data,criterion,compute_loss, get_predicted_steps\n",
    "from post_processing import post_processing_exp,post_processing_gt,shifting_scaling,conv_cord,pose_correction,points_dist\n",
    "from audio_utils import _get_stft_spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12484910,)\n",
      "Audio rate: 48000\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "audio_file_name = \"test_audio\"\n",
    "audio, audio_rate = librosa.load('data/test_audio/{0}.mp3'.format(audio_file_name), sr = 48000)\n",
    "\n",
    "print(audio.shape)\n",
    "actual_audio_rate = audio_rate\n",
    "print(\"Audio rate:\", audio_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audio[audio_rate*20:audio_rate*(20+100)]\n",
    "#audio = audio[44100*22:44100*(22+100)] #only to cheapthrils\n",
    "IPython.display.Audio(audio, rate=audio_rate)\n",
    "librosa.output.write_wav('output/test.wav', audio,audio_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_stft_spectogram(wav_raw, audio_rate):\n",
    "    slope_wav = 0.0144\n",
    "    intersec_wav = 0.8280000000000001\n",
    "    freq = audio_rate\n",
    "    wlen = 640\n",
    "    hop = 320\n",
    "#     if audio_rate == 48000:\n",
    "#         hop = 240\n",
    "#     elif audio_rate == 44100:\n",
    "#         hop = 200\n",
    "#     else:\n",
    "#         raise Exception('Invalid sample rate {0}'.format(audio_rate))    \n",
    "\n",
    "    stft_data = single_spectrogram(wav_raw, freq, wlen, hop) * slope_wav + intersec_wav\n",
    "    return np.swapaxes(stft_data, 0, 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920 2500\n"
     ]
    }
   ],
   "source": [
    "num_secs = math.floor(len(audio)/audio_rate)\n",
    "fps = 25\n",
    "num_secs = num_secs*fps\n",
    "n_audio_rate = int(audio_rate/fps)\n",
    "print(n_audio_rate, num_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "sample_inputs = []\n",
    "for index in range(num_secs):\n",
    "    a = _get_stft_spectogram(audio[n_audio_rate*index: n_audio_rate*(index+1)], actual_audio_rate)\n",
    "#     print(len(a))\n",
    "    sample_inputs.append(a)\n",
    "print(len(sample_inputs))\n",
    "sample_inputs = np.array(sample_inputs)\n",
    "sample_inputs = np.expand_dims(sample_inputs, 1) # for input add channel\n",
    "sample_inputs = np.expand_dims(sample_inputs, 1) # make number of sequences as 1\n",
    "sample_inputs = np.expand_dims(sample_inputs, 1) # make batch_size as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1, 1, 1, 513, 5)\n"
     ]
    }
   ],
   "source": [
    "print(sample_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import PoseMusicDataset_new, CNNFeat, MDNRNN, Nelson_model\n",
    "from model_utils import save_checkpoint, load_checkpoint,load_data,criterion,compute_loss, get_predicted_steps\n",
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_cnt = torch.cuda.device_count()\n",
    "dim = 28\n",
    "z_size = 34\n",
    "n_hidden = 512\n",
    "n_gaussians = 5\n",
    "n_layers = 2\n",
    "if gpu_cnt == 1:\n",
    "    sys.stdout.write(\"One GPU\\n\")\n",
    "    model = MDNRNN(dim, CNNFeat, z_size, n_hidden, n_gaussians, n_layers).cuda()\n",
    "elif gpu_cnt > 1:\n",
    "    sys.stdout.write(\"More GPU's: {0}\\n\".format(gpu_cnt))\n",
    "    model = torch.nn.DataParellel( MDNRNN(dim, CNNFeat, z_size, n_hidden, n_gaussians, n_layers).cuda() )\n",
    "else:\n",
    "    sys.stdout.write(\"No GPU\\n\")\n",
    "    model = MDNRNN(dim, CNNFeat, z_size, n_hidden, n_gaussians, n_layers)\n",
    "    \n",
    "model = model.double()\n",
    "    \n",
    "#criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())#, lr=0.0001, betas=(0.5, 0.999), amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'output/trained_model/Ballet/MDN/latest_epoch.pth.tar'\n",
      "=> loaded checkpoint 'output/trained_model/Ballet/MDN/latest_epoch.pth.tar' (epoch 494)\n"
     ]
    }
   ],
   "source": [
    "#model_saved_path = \"output/motiondance_simplernn/checkpoints/epoch_100_plus_{0}.pth.tar\".format(frozen_after_n_epochs)\n",
    "model_saved_path = \"output/trained_model/Ballet/MDN/latest_epoch.pth.tar\"\n",
    "epoch, model, optimizer = load_checkpoint(model, optimizer, model_saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "output_path = \"output/Result\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1, 1, 1, 513, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:23<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "prev_poses_cnt = 5\n",
    "batch_size = 1\n",
    "prev_poses_input = np.zeros((batch_size, prev_poses_cnt, z_size), dtype=np.float32)\n",
    "cnt = 0\n",
    "tmp_results = []\n",
    "post_proc_results = []\n",
    "print(sample_inputs.shape)\n",
    "\n",
    "for input_index in tqdm(range(0, sample_inputs.shape[0], 100)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         print(sample_inputs.shape)\n",
    "        audio_input = torch.from_numpy(sample_inputs[input_index:input_index+100].reshape(1, 100, 1, 513, 5)).type(torch.DoubleTensor)\n",
    "        prev_poses_input = torch.from_numpy(prev_poses_input).type(torch.DoubleTensor)\n",
    "        model = model.to(device)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        audio_input = audio_input.to(device)\n",
    "        prev_poses_input =  prev_poses_input.to(device)\n",
    "        \n",
    "#         hidden = hidden.to(device)\n",
    "\n",
    "        (pi, mu, sigma), hidden  = model(audio_input, prev_poses_input, hidden)\n",
    "        next_steps = get_predicted_steps(pi, mu)\n",
    "#         print(next_steps.shape)\n",
    "        prev_poses_input = next_steps[:, -prev_poses_cnt:, :]\n",
    "        \n",
    "        #cur_step = np.zeros((1,34), dtype=np.float32)\n",
    "        for seq_index in range(prev_poses_cnt, next_steps.shape[1]):\n",
    "            results = next_steps[:, seq_index, :]\n",
    "            results = results.reshape([17,2])\n",
    "            tmp_results.append(results)\n",
    "#             new_results = conv_cord(results)\n",
    "#             post_proc_results.append(fitting(new_results))\n",
    "\n",
    "\n",
    "\n",
    "np.save('output/MDN_results.npy',tmp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.load('cords/limb_length.npy')\n",
    "neck_hand_leg = np.load('cords/neck_hand_leg.npy')\n",
    "\n",
    "results  = np.load('output/MDN_results.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1259/1259 [00:00<00:00, 2185.69it/s]\n"
     ]
    }
   ],
   "source": [
    "post_processing_gt(results,th_height=0.6,smoothing_len = 5)\n",
    "mdn = np.load('output/Result/gt_modified_5.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:14<00:00, 170.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from helper.utils import create_label_bw, create_label, image_to_video\n",
    "import cv2\n",
    "\n",
    "\n",
    "parts_id = np.load('cords/parts_id.npy')\n",
    "\n",
    "for i in tqdm(range(mdn.shape[0])):\n",
    "    \n",
    "#     pose = pose_correction(cords_exp[i],neck_hand_leg)\n",
    "    points = shifting_scaling(conv_cord(mdn[i]))\n",
    "    label = create_label((512,512,3),points, parts_id)\n",
    "    cv2.imwrite('output/MDN/{:05}.png'.format(i), label)\n",
    "    \n",
    "    label = create_label_bw((512,512),points, parts_id)\n",
    "    cv2.imwrite('output/test_label/{:05}.png'.format(i), label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = Path('./output/MDN/')\n",
    "path_in = str(img_dir)\n",
    "path_out = 'MDN.avi'\n",
    "image_to_video(path_in,path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
